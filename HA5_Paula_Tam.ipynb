{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "672faed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'want', 'to'),\n",
       " ('want', 'to', 'read'),\n",
       " ('to', 'read', 'a'),\n",
       " ('read', 'a', 'book'),\n",
       " ('a', 'book', 'of'),\n",
       " ('book', 'of', 'Shakespeare')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CSC620\n",
    "#HA#5\n",
    "#Paula Abigail Tam <921850992>\n",
    "\n",
    "#first 4 imports have their documentation here: https://www.nltk.org/_modules/nltk/util.html\n",
    "#found documentation on nltk.lm package as a whole at: https://www.nltk.org/api/nltk.lm.html\n",
    "#other documentation on nltk.lm.preprocessing specifically is at: https://www.nltk.org/api/nltk.lm.preprocessing.html\n",
    "\n",
    "from nltk.util import pad_sequence #pads a sequence so there are spaces in between each item\n",
    "    #the parameters are the initial sequence, the ngram degree, boolean if there is a pad to the left or right, and an option symbol to add to the left or right pad\n",
    "from nltk.util import bigrams #This import is what I used for HA#4. This is for bigrams specifically.\n",
    "from nltk.util import ngrams #This import is for any kind of ngram, you can indicate what kind of ngram with the second parameter.\n",
    "from nltk.util import everygrams #This import returns all of the possible ngrams.\n",
    "    #e.g. the string 'a b c' will give [(a), (a, b), (a, b, c), (b), (b, c), (c)]\n",
    "from nltk.lm.preprocessing import pad_both_ends #does the same thing as pad_sequence with all the parameters filled\n",
    "    #more convenient to use pad_both_ends if you do not want to write out all the parameters for pad_sequence\n",
    "from nltk.lm.preprocessing import flatten #makes a list of ngrams into a flat list\n",
    "\n",
    "text = [['I','need','to','book', 'ticket', 'to', 'Australia' ], ['I', 'want', 'to' ,'read', 'a' ,'book', 'of' ,'Shakespeare']]\n",
    "    #text is a tuple with a sentence (as a list of strings) in each element\n",
    "\n",
    "list(bigrams(text[0])) #so text[0] gets the first sentence: ['I','need','to','book', 'ticket', 'to', 'Australia']\n",
    "#print(list(bigrams(text[0])) #gives us:\n",
    "    #[('I', 'need'), ('need', 'to'), ('to', 'book'), ('book', 'ticket'), ('ticket', 'to'), ('to', 'Australia')]\n",
    "\n",
    "list(ngrams(text[1], n=3)) #The n=3 makes it so that the ngrams that are given are trigrams.\n",
    "    #text[1] gives us the second sentence: ['I', 'want', 'to' ,'read', 'a' ,'book', 'of' ,'Shakespeare']\n",
    "    #so all together, this gives us a list of the trigrams of the second sentence.\n",
    "#print(list(ngrams(text[1], n=3))) #gives us:\n",
    "    #[('I', 'want', 'to'), ('want', 'to', 'read'), ('to', 'read', 'a'), ('read', 'a', 'book'), ('a', 'book', 'of'), ('book', 'of', 'Shakespeare')]\n",
    "    \n",
    "#Runnning the cell will just show the list of trigrams, as it is last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86239708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #importing the pandas dataframe\n",
    "df = pd.read_csv('./trump-tweets/realdonaldtrump.csv') #data frame is kinda like a data structure, which means we can treat it like an object\n",
    "    #edited relative path from '../input/trump-tweets/realdonaldtrump.csv' to './trump-tweets/realdonaldtrump.csv' in order to correctly access the dataset\n",
    "df.head() #returns the first n rows (default n = 5) for the object based on position\n",
    "    #the pandas.DataFrame.head documentation can be found at: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html\n",
    "\n",
    "from nltk import word_tokenize, sent_tokenize #for word and sentence tokenization\n",
    "trump_corpus = list(df['content'].apply(word_tokenize)) #the tweets are in 'content' and .apply(word_tokenize) applies the word tokenizer to all the tweets\n",
    "    #trump_corpus is a list of all the tokenized tweets\n",
    "    \n",
    "print(trump_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d97fc53",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m#the pandas.DataFrame.head documentation can be found at: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize, sent_tokenize \u001b[38;5;66;03m#for word and sentence tokenization\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m trump_corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m#the tweets are in 'content' and .apply(word_tokenize) applies the word tokenizer to all the tweets\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m#trump_corpus is a list of all the tokenized tweets\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m padded_everygram_pipeline \u001b[38;5;66;03m#does both everygram and flattens in that order\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4330\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4332\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m-> 1082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1137\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py:130\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/__init__.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/destructive.py:179\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    176\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m text \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp, substitution \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mENDING_QUOTES:\n\u001b[0;32m--> 179\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mregexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubstitution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS2:\n\u001b[1;32m    182\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2 \u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/re.py:325\u001b[0m, in \u001b[0;36m_subx\u001b[0;34m(pattern, template)\u001b[0m\n\u001b[1;32m    322\u001b[0m     template \u001b[38;5;241m=\u001b[39m sre_parse\u001b[38;5;241m.\u001b[39mparse_template(template, pattern)\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sre_parse\u001b[38;5;241m.\u001b[39mexpand_template(template, match)\n\u001b[0;32m--> 325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_subx\u001b[39m(pattern, template):\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;66;03m# internal: Pattern.sub/subn implementation helper\u001b[39;00m\n\u001b[1;32m    327\u001b[0m     template \u001b[38;5;241m=\u001b[39m _compile_repl(template, pattern)\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m template[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(template[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;66;03m# literal replacement\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Separate cell since it is the second example in the blog post\n",
    "\n",
    "#I had to \"pip install opendatasets\" in order to obtain the trump-tweets dataset from Kaggle that the blog was using\n",
    "#I also had to make a Kaggle account in order to download the dataset\n",
    "#I followed this blog to get the dataset into jupyter notebook: https://www.analyticsvidhya.com/blog/2021/04/how-to-download-kaggle-datasets-using-jupyter-notebook/\n",
    "#import opendatasets as od\n",
    "#od.download(\"https://www.kaggle.com/datasets/austinreese/trump-tweets\") #based on the example in the blog we are following, this is the \"trump-tweets\" dataset they used\n",
    "\n",
    "import pandas as pd #importing the pandas dataframe\n",
    "df = pd.read_csv('./trump-tweets/realdonaldtrump.csv') #data frame is kinda like a data structure, which means we can treat it like an object\n",
    "    #edited relative path from '../input/trump-tweets/realdonaldtrump.csv' to './trump-tweets/realdonaldtrump.csv' in order to correctly access the dataset\n",
    "df.head() #returns the first n rows (default n = 5) for the object based on position\n",
    "    #the pandas.DataFrame.head documentation can be found at: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html\n",
    "\n",
    "from nltk import word_tokenize, sent_tokenize #for word and sentence tokenization\n",
    "trump_corpus = list(df['content'].apply(word_tokenize)) #the tweets are in 'content' and .apply(word_tokenize) applies the word tokenizer to all the tweets\n",
    "    #trump_corpus is a list of all the tokenized tweets\n",
    "\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline #does both everygram and flattens in that order\n",
    "# Preprocess the tokenized text for 3-grams language modelling\n",
    "n = 3 #They had this variable n=3 for trigrams. If we wanted to use 4-grams instead, we would only have to change this one variable from 3 to 4\n",
    "    #hooray for reusability!\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, trump_corpus) #train_data and padded_sents are lazy iterators\n",
    "    #so we are padding and flattening the trump_corpus in trigrams (because n=3)\n",
    "\n",
    "from nltk.lm import MLE #MLE stands for Maximum Likelihood Estimator\n",
    "trump_model = MLE(n) # Lets train a 3-grams model, previously we set n=3\n",
    "trump_model.fit(train_data, padded_sents) #.fit trains the model to the text/data given \n",
    "    #since both train_data (the data) and padded_sents (the vocab) are both arguments, this is a supervised model\n",
    "\n",
    "#documentation on the nltk.tokenize.treebank is found at: https://www.nltk.org/_modules/nltk/tokenize/treebank.html\n",
    "#accourding to the documentation, (copy pasted from the page ^), the how it tokenizes is\n",
    "    #- split standard contractions, e.g. ``don't`` -> ``do n't`` and ``they'll`` -> ``they 'll``\n",
    "    #- treat most punctuation characters as separate tokens\n",
    "    #- split off commas and single quotes, when followed by whitespace\n",
    "    #- separate periods that appear at the end of line\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer #the detokenizer uses reverse regex to remove padding, combine contractions, and does not keep the original whitespaces\n",
    "    #e.g. [\"I went\\n to the\\n store.\"] will detokenize to \"I went to the store.\" (if I understood the documentation correctly)\n",
    "    #e.g. the tokenized list [\"I\", \"did\", \"n't\", \"know\", \".\"] will detokenize to \"I didn't know.\"\n",
    "detokenize = TreebankWordDetokenizer().detokenize #another use of reusability, so you dont have to type TreebankWordDetokenizer().detokenize whenever you wanna detokenize\n",
    "\n",
    "def generate_sent(model, num_words, random_seed=42): #parameters are the model we trained, the number of words to print out, and which seed (pick from 0-42)\n",
    "    \"\"\"\n",
    "    :param model: An ngram language model from `nltk.lm.model`.\n",
    "    :param num_words: Max no. of words to generate.\n",
    "    :param random_seed: Seed value for random.\n",
    "    \"\"\"\n",
    "    content = [] #empty list to store the content\n",
    "    for token in model.generate(num_words, random_seed=random_seed): #loop through the generated string that was made based on the trained model\n",
    "            #.generate() generates a string of text based off the trigram models that we trained the model in\n",
    "            #if we did not have the random_seed, then the sentence generated would (most likely) be different everytime\n",
    "        if token == '<s>': #if start of sentence, continue to the next word\n",
    "            continue\n",
    "        if token == '</s>': #if end of sentence, break for loop\n",
    "            break\n",
    "        content.append(token) #append the string into the existing string in the list\n",
    "    return detokenize(content) #return the detokenized version of the list of content\n",
    "\n",
    "generate_sent(trump_model, num_words=10, random_seed=5) #call the function generate_sent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fdfa58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSC620\n",
    "#HA 11.1\n",
    "#Paula Abigail Tam\n",
    "# --- Program 0 ---\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75479e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the toy corpus with 3 documents\n",
    "#and the query string that we will check for\n",
    "\n",
    "#documents\n",
    "doc1 = \"I want to start learning to charge something in life\"\n",
    "doc2 = \"reading something about life no one else knows\"\n",
    "doc3 = \"Never stop learning\"\n",
    "#query string\n",
    "query = \"life learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "177e06c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Document  something  in  learning  I  want  start  to  life  charge\n",
      "0  Term Frequency          1   1         1  1     1      1   2     1       1\n",
      "         Document  something  about  one  else  knows  no  life  reading\n",
      "0  Term Frequency          1      1    1     1      1   1     1        1\n",
      "         Document  Never  learning  stop\n",
      "0  Term Frequency      1         1     1\n"
     ]
    }
   ],
   "source": [
    "#term -frequenvy :word occurences in a document\n",
    "def compute_tf(docs_list):\n",
    "    for doc in docs_list: #for every document in the list\n",
    "        doc1_lst = doc.split(\" \") #split by whitespaces\n",
    "        wordDict_1= dict.fromkeys(set(doc1_lst), 0) #make a dictionary using the set of words in the doc as the key\n",
    "\n",
    "        for token in doc1_lst: #for every word in the document\n",
    "            wordDict_1[token] +=  1 #increment the count of that word by 1, since it's a dictionary, it'll count per word\n",
    "        df = pd.DataFrame([wordDict_1])\n",
    "        idx = 0\n",
    "        new_col = [\"Term Frequency\"]    \n",
    "        df.insert(loc=idx, column='Document', value=new_col)\n",
    "        print(df) #print dataframe\n",
    "        \n",
    "compute_tf([doc1, doc2, doc3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ee1fc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Document  something   in  learning    I  want  start   to  life  \\\n",
      "0  Normalized TF        0.1  0.1       0.1  0.1   0.1    0.1  0.2   0.1   \n",
      "\n",
      "   charge  \n",
      "0     0.1  \n",
      "        Document  something  about    one   else  knows     no   life  reading\n",
      "0  Normalized TF      0.125  0.125  0.125  0.125  0.125  0.125  0.125    0.125\n",
      "        Document     Never  learning      stop\n",
      "0  Normalized TF  0.333333  0.333333  0.333333\n"
     ]
    }
   ],
   "source": [
    "#Normalized Term Frequency\n",
    "def termFrequency(term, document):\n",
    "    normalizeDocument = document.lower().split() #make all the words lowercase\n",
    "    return normalizeDocument.count(term.lower()) / float(len(normalizeDocument)) #count the term over the total length of the document\n",
    "\n",
    "def compute_normalizedtf(documents):\n",
    "    tf_doc = [] #list to store df stuff\n",
    "    for txt in documents:\n",
    "        sentence = txt.split() #split the document into words\n",
    "        norm_tf= dict.fromkeys(set(sentence), 0) #dictionary to keep count\n",
    "        for word in sentence:\n",
    "            norm_tf[word] = termFrequency(word, txt) #gives relative frequency instead of just count\n",
    "        tf_doc.append(norm_tf)\n",
    "        df = pd.DataFrame([norm_tf])\n",
    "        idx = 0\n",
    "        new_col = [\"Normalized TF\"]    \n",
    "        df.insert(loc=idx, column='Document', value=new_col)\n",
    "        print(df) #print the df\n",
    "    return tf_doc #actually returning the list not the dataframe\n",
    "\n",
    "tf_doc = compute_normalizedtf([doc1, doc2, doc3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dfa175d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': 2.09861228866811,\n",
       " 'want': 2.09861228866811,\n",
       " 'to': 2.09861228866811,\n",
       " 'start': 2.09861228866811,\n",
       " 'learning': 1.4054651081081644,\n",
       " 'charge': 2.09861228866811,\n",
       " 'something': 1.4054651081081644,\n",
       " 'in': 2.09861228866811,\n",
       " 'life': 1.4054651081081644,\n",
       " 'reading': 2.09861228866811,\n",
       " 'about': 2.09861228866811,\n",
       " 'no': 2.09861228866811,\n",
       " 'one': 2.09861228866811,\n",
       " 'else': 2.09861228866811,\n",
       " 'knows': 2.09861228866811,\n",
       " 'Never': 2.09861228866811,\n",
       " 'stop': 2.09861228866811}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inverseDocumentFrequency(term, allDocuments):\n",
    "    numDocumentsWithThisTerm = 0\n",
    "    for doc in range (0, len(allDocuments)): #https://www.w3schools.com/python/ref_func_range.asp\n",
    "        #range() makes a sequence with a starting number (0) and an stopping point (len(allDocuments))\n",
    "        #in this case, it acts as the index, goes from 0 to len(allDocuments)-1\n",
    "        if term.lower() in allDocuments[doc].lower().split():\n",
    "            numDocumentsWithThisTerm = numDocumentsWithThisTerm + 1 #increment count if the term is in the document\n",
    " \n",
    "    if numDocumentsWithThisTerm > 0:\n",
    "        return 1.0 + math.log(float(len(allDocuments)) / numDocumentsWithThisTerm) #log the value of allDocuments over count\n",
    "    else:\n",
    "        return 1.0 #or just return 1\n",
    "    \n",
    "def compute_idf(documents):\n",
    "    idf_dict = {} #dictionary\n",
    "    for doc in documents: #for every document\n",
    "        sentence = doc.split() #for every sentence per document\n",
    "        for word in sentence: #for every word in the sentence\n",
    "            idf_dict[word] = inverseDocumentFrequency(word, documents) #count the idf for each word\n",
    "    return idf_dict #return the idf dictionary\n",
    "idf_dict = compute_idf([doc1, doc2, doc3])\n",
    "\n",
    "compute_idf([doc1, doc2, doc3]) #idk why they didnt do print(idf_dict), but this shows the whole dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e06fb1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   doc      life  learning\n",
      "0  0.0  0.140547  0.140547\n",
      "1  1.0  0.175683  0.000000\n",
      "2  2.0  0.000000  0.468488\n"
     ]
    }
   ],
   "source": [
    "# tf-idf score across all docs for the query string(\"life learning\")\n",
    "def compute_tfidf_with_alldocs(documents , query): #comparing the document and the query\n",
    "    tf_idf = []\n",
    "    index = 0\n",
    "    query_tokens = query.split() #the words we are comparing\n",
    "    df = pd.DataFrame(columns=['doc'] + query_tokens) #create dataframe w/ queries as the columns\n",
    "    for doc in documents:\n",
    "        df['doc'] = np.arange(0 , len(documents))\n",
    "        doc_num = tf_doc[index]\n",
    "        sentence = doc.split() #split into sentences\n",
    "        for word in sentence: \n",
    "            for text in query_tokens: #for every word in the sentence, compare with query\n",
    "                if(text == word):\n",
    "                    idx = sentence.index(word)\n",
    "                    tf_idf_score = doc_num[word] * idf_dict[word]\n",
    "                    tf_idf.append(tf_idf_score)\n",
    "                    df.iloc[index, df.columns.get_loc(word)] = tf_idf_score\n",
    "        index += 1 #increment df index\n",
    "    df.fillna(0 , axis=1, inplace=True) #https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html\n",
    "    #filling in any NaN values with 0\n",
    "    return tf_idf , df #tfidf is the list, df is the dataframe version\n",
    "            \n",
    "documents = [doc1, doc2, doc3]\n",
    "tf_idf , df = compute_tfidf_with_alldocs(documents , query)\n",
    "print(df)\n",
    "#print(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bb81c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import Image\n",
    "#Image(\"../input/tfidf-kernel/cosinesimilarity.jpg\")\n",
    "#In the OG code, this image was used to show an example of the queries as vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b26c2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'life': 0.5, 'learning': 0.5}\n"
     ]
    }
   ],
   "source": [
    "#Normalized TF for the query string(\"life learning\")\n",
    "def compute_query_tf(query): \n",
    "    query_norm_tf = {}\n",
    "    tokens = query.split()\n",
    "    for word in tokens:\n",
    "        query_norm_tf[word] = termFrequency(word , query) \n",
    "    return query_norm_tf\n",
    "query_norm_tf = compute_query_tf(query)\n",
    "print(query_norm_tf)\n",
    "\n",
    "#this function gives us the term frequency of just the query string\n",
    "#e.g. if query string(\"life learning is fun\")\n",
    "#output would be {'life': 0.25, 'learning': 0.25, 'is': 0.25, 'fun': 0.25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba11c0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'life': 1.4054651081081644, 'learning': 1.4054651081081644}\n"
     ]
    }
   ],
   "source": [
    "#idf score for the query string(\"life learning\")\n",
    "def compute_query_idf(query):\n",
    "    idf_dict_qry = {}\n",
    "    sentence = query.split()\n",
    "    documents = [doc1, doc2, doc3] #the documents are hard-coded here since it is just a toy corpus\n",
    "    #print(documents)\n",
    "    for word in sentence:\n",
    "        idf_dict_qry[word] = inverseDocumentFrequency(word ,documents)\n",
    "    return idf_dict_qry\n",
    "idf_dict_qry = compute_query_idf(query)\n",
    "print(idf_dict_qry)\n",
    "\n",
    "#this function gives us the idf score of just the query string comparing with the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52e1e8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'life': 0.7027325540540822, 'learning': 0.7027325540540822}\n"
     ]
    }
   ],
   "source": [
    "#tf-idf score for the query string(\"life learning\")\n",
    "def compute_query_tfidf(query):\n",
    "    tfidf_dict_qry = {}\n",
    "    sentence = query.split()\n",
    "    for word in sentence:\n",
    "        tfidf_dict_qry[word] = query_norm_tf[word] * idf_dict_qry[word]\n",
    "        #this function uses the two previous functions and multiplies the results together to get the tfidf score\n",
    "    return tfidf_dict_qry\n",
    "tfidf_dict_qry = compute_query_tfidf(query)\n",
    "print(tfidf_dict_qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14491047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine Similarity(Query,Document1) = Dot product(Query, Document1) / ||Query|| * ||Document1||\n",
    "\n",
    "\"\"\"\n",
    "Example : Dot roduct(Query, Document1) \n",
    "\n",
    "     life:\n",
    "     = tfidf(life w.r.t query) * tfidf(life w.r.t Document1) +  / \n",
    "     sqrt(tfidf(life w.r.t query)) * \n",
    "     sqrt(tfidf(life w.r.t doc1))\n",
    "     \n",
    "     learning:\n",
    "     =tfidf(learning w.r.t query) * tfidf(learning w.r.t Document1)/\n",
    "     sqrt(tfidf(learning w.r.t query)) * \n",
    "     sqrt(tfidf(learning w.r.t doc1))\n",
    "\n",
    "\"\"\"\n",
    "def cosine_similarity(tfidf_dict_qry, df , query , doc_num):\n",
    "    dot_product = 0\n",
    "    qry_mod = 0\n",
    "    doc_mod = 0\n",
    "    tokens = query.split()\n",
    "   \n",
    "    for keyword in tokens:\n",
    "        dot_product += tfidf_dict_qry[keyword] * df[keyword][df['doc'] == doc_num] #the dot product of the query and document\n",
    "        #||Query||\n",
    "        qry_mod += tfidf_dict_qry[keyword] * tfidf_dict_qry[keyword]\n",
    "        #||Document||\n",
    "        doc_mod += df[keyword][df['doc'] == doc_num] * df[keyword][df['doc'] == doc_num]\n",
    "        #to get the absolute value of both the queries and documents\n",
    "        #first the og poster ^2 the value and then square root the result to get a positive value\n",
    "    qry_mod = np.sqrt(qry_mod)\n",
    "    doc_mod = np.sqrt(doc_mod)\n",
    "    #implement formula\n",
    "    denominator = qry_mod * doc_mod #this is the denominator of the cosine similarity\n",
    "    cos_sim = dot_product/denominator #this is the whole cosine similarity\n",
    "     \n",
    "    return cos_sim\n",
    "\n",
    "from collections.abc import Iterable\n",
    "#changed the import from collections to collections.abc\n",
    "#since Iterable from collections import was deprecated\n",
    "def flatten(lis): #this function is just to flatten a list\n",
    "     for item in lis:\n",
    "        if isinstance(item, Iterable) and not isinstance(item, str):\n",
    "             for x in flatten(item): #recursive call\n",
    "                yield x\n",
    "        else:        \n",
    "             yield item #yeild can return a sequence of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1a9dbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Document1', 'Document2', 'Document3']\n",
      "[1.0, 0.7071067811865475, 0.7071067811865475]\n"
     ]
    }
   ],
   "source": [
    "def rank_similarity_docs(data):\n",
    "    cos_sim =[]\n",
    "    for doc_num in range(0 , len(data)):\n",
    "        cos_sim.append(cosine_similarity(tfidf_dict_qry, df , query , doc_num).tolist())\n",
    "    return cos_sim\n",
    "similarity_docs = rank_similarity_docs(documents)\n",
    "doc_names = [\"Document1\", \"Document2\", \"Document3\"]\n",
    "print(doc_names)\n",
    "print(list(flatten(similarity_docs))) #need to flatten the list since similarity_docs is a list of lists"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c63c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import opendatasets as od\n",
    "#od.download(\"https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification\")\n",
    "#import nltk\n",
    "#nltk.download('wordnet') \n",
    "#nltk.download('omw-1.4')\n",
    "#skip the blocks #27-#32 and #90 \n",
    "\n",
    "import re #regex\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from nltk.tokenize import TweetTokenizer #tokenizer for tweets, documentation is found here: https://www.nltk.org/api/nltk.tokenize.casual.html\n",
    "from nltk.stem import WordNetLemmatizer #lemmatizes words, documentation here: https://www.nltk.org/_modules/nltk/stem/wordnet.html\n",
    "from nltk.corpus import stopwords #this is used to remove any stopwords / filler words\n",
    "from nltk.stem import PorterStemmer #stemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer #matrix of token counts: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #equivalent to CountVectorizer then TfidfTransformer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "from sklearn.model_selection import train_test_split #split arrays/matrices into random train and test subsets: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "from sklearn.ensemble import RandomForestClassifier #this is not needed since it is the section we are skipping\n",
    "from sklearn.model_selection import cross_val_score #this isn't used in the og code, so idk what they intended to use this for\n",
    "    #however it could be used to get a score by cross validation\n",
    "from sklearn.metrics import confusion_matrix #this allows me to create a confusion matrix\n",
    "#import xgboost \n",
    "    #I keep getting an error that says \"ModuleNotFoundError: No module named 'xgboost'\"\n",
    "    #I kept getting this error after installing it in my conda enviroment\n",
    "    #When trying to use \"pip install xgboost\" to install this module, I kept getting an error that starts with \"ERROR: Command errored out with exit status 1:\"\n",
    "    #followed by a VERY long error message followed by \n",
    "        #ERROR: Failed building wheel for xgboost\n",
    "        #Running setup.py clean for xgboost\n",
    "        #Failed to build xgboost\n",
    "        #Installing collected packages: xgboost\n",
    "        #Running setup.py install for xgboost ... error\n",
    "    #and then it is followed by \"error: [Errno 2] No such file or directory: 'cmake'\"\n",
    "from sklearn.model_selection import RandomizedSearchCV #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4cb81e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"./covid-19-nlp-text-classification/Corona_NLP_train.csv\",encoding='latin1')\n",
    "#changed the path from \"/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_train.csv\"\n",
    "#we are using pandas to read the csv\n",
    "#the encoding allows us to read latin characters (e.g. ê, æ, etc)\n",
    "#found this website that shows whats in Latin1: https://kb.iu.edu/d/aepu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdd0ff17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "3      3802       48754        NaN  16-03-2020   \n",
       "4      3803       48755        NaN  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
       "1  advice Talk to your neighbours family to excha...            Positive  \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive  \n",
       "3  My food stock is not the only one which is emp...            Positive  \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head() #used to see the first 5 rows of the .csv to check if it is the right one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8710c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop(p):\n",
    "    p.drop([\"UserName\",\"ScreenName\",\"Location\",\"TweetAt\"],axis=1,inplace=True)\n",
    "    #this function removes the UserName, ScreenName, Location, and TweetAt columns and keeps the rest of the columns in order\n",
    "    #documentation can be found here: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0cd1105",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop(train) #use the drop function on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "806d1269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral\n",
       "1  advice Talk to your neighbours family to excha...            Positive\n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive\n",
       "3  My food stock is not the only one which is emp...            Positive\n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head() #check mutated train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc5a8814",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweettoken = TweetTokenizer(strip_handles=True, reduce_len=True) #tokenizes the tweets\n",
    "#strip_handles removes handles and reduce_len reduces the length of 3+ repeated characters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca432a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer() #lemmatizer for word net words: https://www.nltk.org/_modules/nltk/stem/wordnet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41c5a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer() #the porter stemmer: https://www.nltk.org/_modules/nltk/stem/porter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24cc2f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect=[] #list to store the preprocess function output thing\n",
    "def preprocess(t):\n",
    "    tee=re.sub('[^a-zA-Z]',\" \",t) #replace anything that begins with a alpha character with a whitespace: https://docs.python.org/3/library/re.html\n",
    "    tee=tee.lower() #make it all lowercase\n",
    "    res=tweettoken.tokenize(tee) #tokenize the tweets --> becomes a list\n",
    "    for i in res:\n",
    "        if i in stopwords.words('english'): #remove the stopwords\n",
    "            res.remove(i)\n",
    "    rest=[]\n",
    "    for k in res:\n",
    "        rest.append(lemmatizer.lemmatize(k)) #lemmatize the res list and shove it into the rest list\n",
    "    ret=\" \".join(rest) #join rest together with whitespaces in between\n",
    "    collect.append(ret) #put this all into the the collect list outside of this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5cea8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(41157): #b the range is literally 41k, it takes forever (a couple of minutes) to do this function\n",
    "    preprocess(train[\"OriginalTweet\"].iloc[j])\n",
    "    #needed to download nltk.wordnet using nltk.download('wordnet') and nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e4bc9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['menyrbie phil gahan chrisitv http co ifz fan pa http co xx ghgfzcc http co nlzdxno',\n",
       " 'advice talk your neighbour family exchange phone number create contact list phone number neighbour school employer chemist gp set online shopping account po adequate supply regular med not order',\n",
       " 'coronavirus australia woolworth give elderly disabled dedicated shopping hour amid covid outbreak http co binca vp p',\n",
       " 'food stock not only one is empty please panic will enough food everyone not take than you need stay calm stay safe covid france covid covid coronavirus confinement confinementotal confinementgeneral http t co zrlg z j',\n",
       " 'ready go supermarket covid outbreak because m paranoid because food stock litteraly empty the coronavirus a serious thing please panic cause shortage coronavirusfrance restezchezvous stayathome confinement http t co usmualq n']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect[:5] #the :5 shows the first 5 in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dabc24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(ll): #parameter is double lowercase l, maybe for \"lemmatized list\" or smth\n",
    "    cv=CountVectorizer(max_features=200) #limit the max features of the vector to 200\n",
    "    x=cv.fit_transform(ll).toarray() #both the .fit and .transform functions together, returns a matrix --> .toarray() makes it an array\n",
    "    #documentation for fit_transform: https://github.com/scikit-learn/scikit-learn/blob/36958fb24/sklearn/feature_extraction/text.py#L1294\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24fe8b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=bow(collect) #using the bow function on the collect list and setting it to y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "620f3154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:1] #this shows the first vector of y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20a8588d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y[0][:]) #this shows the length of the first vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e4ccf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(xx): ##tfidf refers to TF-IDF features which is \"term frequency-inverse document frequency\"\n",
    "    #statistical measure that evaluates how relevant a word is to a document in a collection of documents\n",
    "    cv=TfidfVectorizer(max_features=4000) #max features set to 4000\n",
    "    x=cv.fit_transform(xx).toarray() #similar to what happened with the bow function\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3e7c285",
   "metadata": {},
   "outputs": [],
   "source": [
    "values=train[\"Sentiment\"].values #get the values of the sentiment column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c82b9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Neutral', 'Positive', 'Positive', ..., 'Positive', 'Neutral',\n",
       "       'Negative'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values #the implementation we are following has the results as \"array([2, 0, 0, ..., 0, 2, 1])\"\n",
    "#but the values that I get here are the literal strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b3dbe5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41157, 200)\n",
      "(41157,)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(values.shape)\n",
    "#these were to double check the lengths of these arrays\n",
    "#since at first I got an error when trying to use train_test_split bc the size of y and values did not match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95b24bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, x_test, y_train, y_test) = train_test_split(y, values, train_size=0.75, random_state=42) #make random train and test sets\n",
    "#double checked the lengths of both y and values using .shape\n",
    "#documentation was found here: https://numpy.org/devdocs/reference/generated/numpy.shape.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c9e67d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train #look at the x values of the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4005e2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB #multinomial Naïve Bayes\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train,y_train)  #fit the naive bayes classifier with the training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "733ba413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3825072886297376"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_test,y_test) #need to compare this value with the other score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4f7ebcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 435   58  384  200  240]\n",
      " [  71  670  153  233  516]\n",
      " [ 380  177  769  601  586]\n",
      " [  65   62  325 1061  406]\n",
      " [ 174  502  512  709 1001]]\n"
     ]
    }
   ],
   "source": [
    "#I followed this stackoverflow answer to figure out how to print out a confusion matrix for Naïve Bayes\n",
    "#https://stackoverflow.com/questions/48412360/how-can-i-use-confusion-matrix-with-naive-bayes-in-python\n",
    "y_pred = clf.predict(x_test) #so we can get the predictions with the x test set\n",
    "con_mat = confusion_matrix(y_test, y_pred) #compare the actual test with the predictions\n",
    "print(con_mat) #this is the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3bae16f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=tfidf(collect) #use the tfidf function on the collect list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "894f319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,x_test,y_train,y_test) = train_test_split(y,values, train_size=0.75, random_state=42) #make random train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9884406c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46763848396501456"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(x_train,y_train) #same as before\n",
    "clf.score(x_test,y_test) #need to compare this score with the previous one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98eadd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 262    2  807   32  214]\n",
      " [   2  415   78   34 1114]\n",
      " [  94   26 1295  186  912]\n",
      " [   8   20  350  774  767]\n",
      " [  15  131  491  195 2066]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(x_test) #I basically just copy-pasted the same code I used for the first confusion matrix\n",
    "con_mat = confusion_matrix(y_test, y_pred)\n",
    "print(con_mat) #this is the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a113d97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. The output the .score() function provides is the accuracy of the current classifier.\n",
    "#I found a good explanation about .score() at https://www.kaggle.com/getting-started/27261\n",
    "\n",
    "#2. The reason why that the second score is bigger than the first score is because CountVectorizer is just counting the frequency of the words.\n",
    "# TfidfVectorizer however does not only count the frequency of the words, but also assigns importance (tfidf score) to each word.\n",
    "#e.g. If a word was used too rarely, it would be given low tfidf score. This affects the overall score when computed.\n",
    "#I found this comparison in this blog post: https://www.linkedin.com/pulse/count-vectorizers-vs-tfidf-natural-language-processing-sheel-saket/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
